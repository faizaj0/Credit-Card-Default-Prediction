{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "from IPython.display import display\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "#Preprocessing and Visual\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# RANDOM STATE 42 USED THROUGHOUT SO VALUES ARE CONSISTENT WITH REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset via pandas\n",
    "FILE_NAME = \"data.csv\" #PLEASE CHANGE\n",
    "\n",
    "main_df = pd.read_csv(FILE_NAME) #for reference\n",
    "df = pd.read_csv(FILE_NAME) #file to change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column Categories - Easier to refer too\n",
    "all_questions = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
    "       'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21',\n",
    "       'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31',\n",
    "       'Q32']\n",
    "\n",
    "\n",
    "training_features = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
    "       'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q18', 'Q19', 'Q20', 'Q21',\n",
    "       'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31',\n",
    "       'Q32', 'age']\n",
    "\n",
    "humour_scores = ['affiliative', 'selfenhancing', 'agressive', 'selfdefeating']\n",
    "\n",
    "user_characteristics = ['age', 'gender', 'accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape)\n",
    "display(df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing 0 accuracy responses, gender and humour scores, storing accuracy values\n",
    "\n",
    "df = df[df.accuracy != 0] \n",
    "print(f\"These many instances removed: {len(df) - len(main_df)}\")\n",
    "\n",
    "df.drop(humour_scores+[\"gender\",\"accuracy\"], axis = 1, inplace = True, errors = \"ignore\")\n",
    "display(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No blank values (Nan; excluding -1 values)\n",
    "display(f\"Null Values present?: {set(df.isnull().any())}\")\n",
    "#Data Type:\n",
    "display(f\"Data Types present: {set(df.dtypes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apparent Range\n",
    "\n",
    "\n",
    "display(f\"Number of values for questions : {len(df.Q1.unique())}, therefore can be considered Ordinal since also integer\")\n",
    "display(f\"Number of values for questions excluding -1: {len(df[df.Q1 != -1].Q1.unique())}, therefore can be considered Ordinal\")\n",
    "display(f\"Number of unique age values: {len(df.age.unique())}, large number therefore can be considered continous\")\n",
    "\n",
    "\n",
    "\n",
    "#Range is consitent throughout all questions. According to codebook however -1 indicates unanaswered response \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Finding Age outliers\n",
    "\n",
    "\n",
    "display(df.age.describe())\n",
    "#max value is over 122... considered outlier\n",
    "\n",
    "sns.boxplot(main_df.age)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "age_value_counts = df.age.value_counts()\n",
    "sums = 0\n",
    "max_age = 122\n",
    "\n",
    "for value in age_value_counts.keys():\n",
    "    if value > max_age:\n",
    "        print(value,\":\", age_value_counts[value])\n",
    "        sums += age_value_counts[value]\n",
    "        \n",
    "print(f\"{sums} value(s) over 122\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing responses per column, checking to see if we remove column\n",
    "max_missing = 0\n",
    "column_label = \"\"\n",
    "\n",
    "for column in df[all_questions].columns:\n",
    "\n",
    "    if -1 in df[column].value_counts().keys() and (df[column].value_counts()[-1]) > max_missing :   #IF COLUMN HAS -1 AND IF GREATER THAN WORST\n",
    "            max_missing = (df[column].value_counts()[-1])\n",
    "            column_label = column\n",
    "\n",
    "f\"Column with most amount of blanks: {column_label} with {max_missing} missing in column, insignificant amount\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rows with multiple missing values removed\n",
    "\n",
    "per_column_missing_value = {} #FOR LATER USE TO REMOVE VALUES SPECIFIC TO COLUMN\n",
    "\n",
    "minus_indexs = []\n",
    "for column in df[all_questions]: #ITERATE THROUGH COLUMNS\n",
    "    \n",
    "    minus_indexs.extend(df[df[column] == float(-1)].index) #APPENDING INDEXES \n",
    "    per_column_missing_value[column] = df[df[column] == float(-1)].index \n",
    "\n",
    "\n",
    "#Key is number of missing entries in row - value is how many of those.\n",
    "# For e.g 67 rows have at least 1 missing value\n",
    "\n",
    "\n",
    "NUMBER_OF_MISSING_PER_INDEX = Counter(minus_indexs)\n",
    "\n",
    "NUMBER_OF_MISSING_COUNT = Counter(NUMBER_OF_MISSING_PER_INDEX.values())\n",
    "\n",
    "more_than_4_missing = 0  #Number of entries with more than 4 unaswered\n",
    "OVER_LIMIT = 4 #Threshold\n",
    "\n",
    "for key in NUMBER_OF_MISSING_COUNT.keys():\n",
    "    if key > OVER_LIMIT: \n",
    "        more_than_4_missing += NUMBER_OF_MISSING_COUNT[key]\n",
    "\n",
    "display(f\"{more_than_4_missing} entries had more than 4 (> 4) missing values which will be removed\")   #Including Q17 -> Q17 and three more or 4 other\n",
    "\n",
    "Q17_MINUS_INDEXS = df[df[\"Q17\"] == float(-1)].index   #ALL MISSING VALUES IN Q17 TO BE REMOVED -> AVOID UNCERTAINTY\n",
    "\n",
    "display(f\"Q17 had {len(Q17_MINUS_INDEXS)} blanks which will be removed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "\n",
    "\n",
    "for key in NUMBER_OF_MISSING_PER_INDEX.keys():    \n",
    "    if NUMBER_OF_MISSING_PER_INDEX[key] > 4:\n",
    "        to_remove.append(key)\n",
    "\n",
    "\n",
    "to_remove.extend(Q17_MINUS_INDEXS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(f\"{len(to_remove)} will be removed since they either contain more than 4 missing values and/or have Q17 missing\")\n",
    "\n",
    "df.drop(list(set(to_remove)), axis = 0, inplace = True, errors = \"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FUTURE WHEN WE REMOVE ROWS SPECIFIC TO COLUMNS\n",
    "# WE REMOVE FROM DICTIONARY\n",
    "\n",
    "# to_remove\n",
    "\n",
    "for key in per_column_missing_value.keys():\n",
    "    per_column_missing_value[key] = per_column_missing_value[key].to_list()\n",
    "\n",
    "\n",
    "\n",
    "for key in per_column_missing_value.keys():\n",
    "    \n",
    "    for term in to_remove:\n",
    "        if term in per_column_missing_value[key]:\n",
    "            per_column_missing_value[key].remove(term)\n",
    "\n",
    "\n",
    "# per_column_missing_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column is encoded, showing distrubtion\n",
    "\n",
    "df[\"Binary_Q17\"] = pd.cut(df[\"Q17\"], bins = [0,1,5], labels = [0,1])\n",
    "BINARY_Q17_VALUE_COUNTS = df[\"Binary_Q17\"].value_counts(normalize= True)\n",
    "\n",
    "#Dropping Q17 column\n",
    "\n",
    "df.drop(\"Q17\", axis = 1, inplace=True, errors = \"ignore\")\n",
    "display(BINARY_Q17_VALUE_COUNTS)\n",
    "\n",
    "plt.pie([BINARY_Q17_VALUE_COUNTS[0],BINARY_Q17_VALUE_COUNTS[1]], labels = [0,1],autopct='%0.01f%%', startangle=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITING DATA SET\n",
    "\"NEEDS TO BE CHANGED\"\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(df[training_features], df[\"Binary_Q17\"], test_size=0.2, random_state=42,stratify=df[\"Binary_Q17\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Values for whole dataset Train and Test\n",
    "\n",
    "training_set_question_modes = X_train.mode().drop(\"age\", axis = 1) #TRAINING SET MODE\n",
    "training_set_age_median = X_train.age.median() #TRAINING SET AGE MEDIAN\n",
    "\n",
    "\n",
    "#Replace values across 3 datasets, X_train, X_test, df\n",
    "\n",
    "for dataset in X_train, X_test, df:\n",
    "\n",
    "    for column in training_features[:-1]: # Skip age column\n",
    "        dataset[column] = np.where(dataset[column] == -1, training_set_question_modes[column], dataset[column])\n",
    "\n",
    "    dataset[\"age\"] = np.where(dataset.age > 122, training_set_age_median, dataset.age)\n",
    "\n",
    "\n",
    "# # list(training_set_question_modes.values.ravel())\n",
    "# list(testing_set_question_modes.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age analysis\n",
    "\n",
    "\n",
    "\n",
    "kolmogorov_smirnov_results = ks_2samp(df[df[\"Binary_Q17\"]==0][\"age\"], df[df[\"Binary_Q17\"]==1][\"age\"])\n",
    "kolmogorov_smirnov_results\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(df[df[\"Binary_Q17\"]==0][\"age\"], label=\"Binary_Q17: 0\", ax=ax)\n",
    "sns.kdeplot(df[df[\"Binary_Q17\"]==1][\"age\"], label=\"Binary_Q17: 1\", ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Age\"), ax.set_ylabel(\"Density\"), ax.legend(), ax.set_title(\"Age distribtions specific to Q17_Binary\")\n",
    "\n",
    "plt.text(60, 0.035, \"Kolmogorov-Smirnov test: \", ha='center', va='bottom',weight = \"bold\")\n",
    "plt.text(60, 0.03, f\"Statistic: {round(kolmogorov_smirnov_results.statistic, 3)}\" ,ha='left', va='bottom')\n",
    "plt.text(60, 0.026,f\"P-Value: {round(kolmogorov_smirnov_results.pvalue, 3)}\", ha='left', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Will drop age column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question Analysis\n",
    "\n",
    "\n",
    "stats_results = []\n",
    "\n",
    "for value in training_features[:-1]: #all except for last column\n",
    "    correlation, pval = spearmanr(df[[value, \"Binary_Q17\"]])\n",
    "    if pval < 0.05:\n",
    "        stats_results.append([value,correlation])\n",
    "    \n",
    "\n",
    "stats_results = sorted(stats_results, key = lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "NUMBER_OF_GRAPHS = 2\n",
    "DIMENSIONS = 4,4\n",
    "\n",
    "fig, ax =plt.subplots(1,NUMBER_OF_GRAPHS)\n",
    "fig.set_size_inches(DIMENSIONS[0]*NUMBER_OF_GRAPHS, DIMENSIONS[1])\n",
    "\n",
    "for i, value in enumerate([value[0] for value in stats_results[0:NUMBER_OF_GRAPHS]]):\n",
    "\n",
    "    correlation, pval = spearmanr(df[[value, \"Binary_Q17\"]])\n",
    "    # print(f'{value}: correlation={correlation:.6f}, p-value={pval:.6f}')\n",
    "\n",
    "    sns.boxplot(x = \"Binary_Q17\", y =value, data = df, ax = ax[i])\n",
    "    # ax[i].set_title(value, weight = \"bold\")\n",
    "    \n",
    "    ax[i].text(0, 5.2, f\"Spearman: {round(correlation,3)} \", ha='center', va='bottom', weight = \"bold\")\n",
    "\n",
    "    # display(pd.crosstab(df[value],df[\"Binary_Q17\"]))\n",
    "    # print(\"------\\n\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Top 5 values have absolute correlation over 5 with a pvalue < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns_replaced_values = []\n",
    "\n",
    "\n",
    "for key in \"Q21,Q1,Q25,Q5,Q29\".split(\",\"):\n",
    "    selected_columns_replaced_values.extend(per_column_missing_value[key])\n",
    "\n",
    "\n",
    "INDEXS_COUNT = Counter(selected_columns_replaced_values)\n",
    "\n",
    "\n",
    "amount = Counter((INDEXS_COUNT).values())\n",
    "TOTAL = 0\n",
    "for key in amount.keys(): \n",
    "    if key > 1: \n",
    "        TOTAL += amount[key]\n",
    "\n",
    "print(INDEXS_COUNT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(f\"{TOTAL} needs to be removed from the dataset since these were replaced and since so few values its unlikely itll hinder statistical significance of previous tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove entries with more than 2 blank values (reduce uncertaninty) from both test and train set\n",
    "to_remove = []\n",
    "\n",
    "for key in INDEXS_COUNT.keys(): \n",
    "    if INDEXS_COUNT[key] > 1:\n",
    "        print(key)\n",
    "        to_remove.append(key)\n",
    "\n",
    "print(to_remove)\n",
    "print(\"Above indexes to be removed since they contained more than 1 replaced values\")\n",
    "\n",
    "for value in to_remove:\n",
    "    X_train.drop(value, errors = \"ignore\", inplace = True)\n",
    "    y_train.drop(value, errors = \"ignore\", inplace = True)\n",
    "    X_test.drop(value, errors = \"ignore\", inplace = True)\n",
    "    y_test.drop(value, errors = \"ignore\", inplace = True)\n",
    "\n",
    "\n",
    "# per_column_missing_value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[np.array(stats_results[:5])[:,0]]\n",
    "pd.concat([X_train, y_train], axis=1).head()\n",
    "\n",
    "Scalar = MinMaxScaler()\n",
    "Scalar.fit(X_train) #USING TRAIN DATA ONLY ALTHOUGH SHOULD BE SAME RANGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pd.DataFrame(Scalar.transform(X_train), columns = X_train.columns)\n",
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_dm(func, alpha = 0.5, hue = y_train, **kwargs):\n",
    "\n",
    "   \n",
    "    arg_func = func(**kwargs)\n",
    "    arg_func.fit(df)\n",
    "    \n",
    "    varimax_components = arg_func.fit_transform(X_train)\n",
    "\n",
    "    # sns.scatterplot( x = varimax_components[:,0],hue=hue,y = varimax_components[:,1],alpha=alpha)\n",
    "    # plt.show()\n",
    "    print(\"-----\")\n",
    "\n",
    "    return arg_func\n",
    "\n",
    "analysis = apply_dm(PCA, alpha = 0.8)\n",
    "np.cumsum(analysis.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# apply_dm(FactorAnalysis, alpha = alpha, rotation = \"varimax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training SVM GRID SEARCH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# C_values = [0.001,0.005,0.01,0.05,0.1,5,10,50,100,500,1000]\n",
    "\n",
    "\n",
    "\n",
    "C_values = [0.001,0.1,10,1000]\n",
    "\n",
    "tuned_parameters = [\n",
    "    {\"kernel\": [\"rbf\"], \"gamma\": [1e2,1e1,1e-0,1e-1,1e-2], \"C\": C_values, \"class_weight\": [\"balanced\"]},\n",
    "    {\"kernel\": [\"linear\"], \"C\": C_values,  \"class_weight\": [\"balanced\"]},\n",
    "    {\"kernel\": [\"poly\"], \"degree\": [2,3,4], \"C\": C_values,  \"class_weight\": [\"balanced\"]},\n",
    "    \n",
    "]\n",
    "\n",
    "# param_grid_SVM = {\n",
    "#     'kernel': [\"linear\", \"rbf\",\"poly\"],\n",
    "#     'C': [0,0.01,1,100],\n",
    "#     \"degree\": [2,3],\n",
    "#     'gamma': [0.01,0.1,10,100],\n",
    "#     'class_weight': [\"balanced\"]\n",
    "# }\n",
    "\n",
    "#BY DEFAULT STRATIFIED CROSS VALIDATOIN NO NEED TO SPECIFY\n",
    "\n",
    "grid_search_svm = GridSearchCV(SVC(),tuned_parameters, scoring = \"roc_auc\",cv = 10, return_train_score=True, n_jobs = -1, refit = False, verbose = 1) \n",
    "\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search_svm_results = pd.DataFrame(grid_search_svm.cv_results_)\n",
    "\n",
    "temp = pd.melt(grid_search_svm_results, id_vars=[\"param_kernel\"], value_vars=['mean_test_score', 'mean_train_score'])\n",
    "\n",
    "temp.rename(columns = {\"value\": \"AUC_Score\"}, inplace = True)\n",
    "sns.boxplot(data = temp, x = \"param_kernel\", y = \"AUC_Score\", hue = \"variable\")\n",
    "plt.show()\n",
    "# sns.boxplot(data = grid_search_svm_results, x = \"param_kernel\", y = \"mean_test_score\")\n",
    "# plt.show()\n",
    "# sns.boxplot(data = grid_search_svm_results, x = \"param_kernel\", y = \"mean_test_score\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "temp = grid_search_svm_results[grid_search_svm_results.param_kernel == \"linear\"]\n",
    "\n",
    "temp = pd.melt(temp, id_vars=[f\"param_C\"], value_vars=['mean_test_score', 'mean_train_score']).rename(columns = {\"value\": \"AUC_Score\"})\n",
    "# temp.rename(columns = {\"value\": \"AUC_Score\"}, inplace = True)\n",
    "\n",
    "grid = sns.lineplot(data = temp, x = f\"param_C\", y = \"AUC_Score\", hue = \"variable\")\n",
    "grid.set(xscale=\"log\")\n",
    "grid.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERGING ON C PARAMATERS\n",
    "\n",
    "\n",
    "#Model Training SVM GRID SEARCH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "C_values = [0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000]\n",
    "\n",
    "\n",
    "\n",
    "tuned_parameters = [\n",
    "    {\"kernel\": [\"linear\"], \"C\": C_values,  \"class_weight\": [\"balanced\"]}    \n",
    "]\n",
    "\n",
    "# param_grid_SVM = {\n",
    "#     'kernel': [\"linear\", \"rbf\",\"poly\"],\n",
    "#     'C': [0,0.01,1,100],\n",
    "#     \"degree\": [2,3],\n",
    "#     'gamma': [0.01,0.1,10,100],\n",
    "#     'class_weight': [\"balanced\"]\n",
    "# }\n",
    "\n",
    "#BY DEFAULT STRATIFIED CROSS VALIDATOIN NO NEED TO SPECIFY\n",
    "\n",
    "grid_search_svm = GridSearchCV(SVC(),tuned_parameters, scoring = \"roc_auc\",cv = 10, return_train_score=True, n_jobs = -1, refit = False, verbose = 1) \n",
    "\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid_search_svm.best_params_)\n",
    "display(grid_search_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for iterations in range(100,2000,20):\n",
    "    clf = SVC(**grid_search_svm.best_params_, max_iter = iterations)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10, scoring= \"roc_auc\")\n",
    "    x.append(iterations)\n",
    "    y.append(scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.lineplot(x,y)\n",
    "fig.set_ylim(0.5,0.89)\n",
    "fig.set(xlabel='iterations', ylabel='score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "\n",
    "\n",
    "grid_values = {\"max_depth\": list(range(1,30,6)),\n",
    "                \"n_estimators\": [1,10,100,200,500],\n",
    "                'min_samples_split': range(1,30,5),\n",
    "              \"min_samples_leaf\": range(1,30,5),\n",
    "              \"bootstrap\": [True],\n",
    "              \"class_weight\": [\"balanced\"], \n",
    "              \"criterion\": [\"gini\",\"entropy\"] \n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "grid_search_rfc = GridSearchCV(RandomForestClassifier(random_state = 42), param_grid = grid_values, scoring = 'roc_auc',cv = 10, return_train_score=True, n_jobs = -1, verbose = 10)\n",
    "\n",
    "grid_search_rfc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# [5,10,15,20,25,35]\n",
    "# [4,5,6,7,8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(grid_search_rfc.cv_results_)[[\"params\", \"mean_test_score\"]].sort_values(by = \"mean_test_score\", ascending = False)\n",
    "\n",
    "\n",
    "grid_search_rfc_results = pd.DataFrame(grid_search_rfc.cv_results_)\n",
    "\n",
    "\n",
    "for variable in [\"param_max_depth\",\"param_n_estimators\",\"param_min_samples_split\",\"param_min_samples_leaf\",\"param_max_depth\"]:\n",
    "\n",
    "    temp = pd.melt(grid_search_rfc_results, id_vars=[variable], value_vars=['mean_test_score', 'mean_train_score']).rename(columns = {\"value\": \"AUC_Score\"})\n",
    "    sns.lineplot(data = temp, x = variable, y = \"AUC_Score\", hue = \"variable\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "\n",
    "\n",
    "grid_values = {\"max_depth\": list(range(1,10,1)),\n",
    "                \"n_estimators\": [100,200],\n",
    "                'min_samples_split': [10,20],\n",
    "              \"min_samples_leaf\": [10,20],\n",
    "              \"bootstrap\": [True],\n",
    "              \"class_weight\": [\"balanced\"], \n",
    "              \"criterion\": [\"gini\"] \n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "grid_search_rfc = GridSearchCV(RandomForestClassifier(random_state = 42), param_grid = grid_values, scoring = 'roc_auc',cv = 10, return_train_score=True, n_jobs = -1, verbose = 10)\n",
    "\n",
    "grid_search_rfc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# [5,10,15,20,25,35]\n",
    "# [4,5,6,7,8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid_search_rfc.best_params_)\n",
    "display(grid_search_rfc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "n_neighbors = list(range(1,500,50))\n",
    "p=[1,2]\n",
    "weights = [\"uniform\", \"distance\"]\n",
    "#Convert to dictionary\n",
    "param_grid = dict(n_neighbors=n_neighbors, p=p, weights = weights)\n",
    "\n",
    "  \n",
    "# defining parameter range\n",
    "grid_k = GridSearchCV(knn, param_grid, cv=10, scoring='roc_auc', return_train_score=True,verbose=10, n_jobs = -1)\n",
    "\n",
    "\n",
    "grid_k.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_knn_results = pd.DataFrame(grid_k.cv_results_)\n",
    "\n",
    "\n",
    "temp = pd.melt(grid_knn_results, id_vars=[\"param_weights\"], value_vars=['mean_test_score', 'mean_train_score'])\n",
    "temp\n",
    "sns.boxplot(data = temp, x = \"param_weights\", y = \"value\", hue = \"variable\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# grid_knn_results = grid_knn_results[grid_knn_results.param_weights == \"uniform\"]\n",
    "# temp = pd.melt(grid_knn_results, id_vars=[\"param_p\"], value_vars=['mean_test_score', 'mean_train_score'])\n",
    "\n",
    "# sns.boxplot(data = temp, x = \"param_p\", y = \"value\", hue = \"variable\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "grid_knn_results = grid_knn_results[grid_knn_results.param_weights == \"uniform\"]\n",
    "temp = pd.melt(grid_knn_results, id_vars=[\"param_n_neighbors\"], value_vars=['mean_test_score', 'mean_train_score'])\n",
    "\n",
    "sns.lineplot(data = temp, x = \"param_n_neighbors\", y = \"value\", hue = \"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# temp = grid_knn_results[grid_knn_results.param_weights == \"uniform\"]\n",
    "# temp = temp[temp.param_p == 1]\n",
    "\n",
    "\n",
    "# temp = pd.melt(temp, id_vars=[\"param_n_neighbors\"], value_vars=['mean_test_score', 'mean_train_score']).rename(columns = {\"value\": \"AUC_Score\"})\n",
    "\n",
    "\n",
    "\n",
    "# sns.lineplot(data = temp, x = \"param_n_neighbors\", y = \"AUC_Score\", hue = \"variable\")\n",
    "\n",
    "# display(grid_k.best_params_)\n",
    "display(grid_k.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "n_neighbors = list(range(1,500,1))\n",
    "p=[1]\n",
    "weights = [\"uniform\"]\n",
    "#Convert to dictionary\n",
    "param_grid = dict(n_neighbors=n_neighbors, p=p, weights = weights)\n",
    "\n",
    "  \n",
    "# defining parameter range\n",
    "grid_k = GridSearchCV(knn, param_grid, cv=10, scoring='roc_auc', return_train_score=True,verbose=10, n_jobs = -1)\n",
    "\n",
    "\n",
    "grid_k.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid_k.best_params_)\n",
    "display(grid_k.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaled_X_test = pd.DataFrame(Scalar.transform(X_test[X_train.columns]), columns = X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# knn = KNeighborsClassifier(**{'n_neighbors': 89, 'p': 1, 'weights': 'uniform'})\n",
    "knn = KNeighborsClassifier(**grid_k.best_params_)\n",
    "\n",
    "# svm = SVC(**{'C': 50, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'})\n",
    "svm = SVC(**grid_search_svm.best_params_)\n",
    "\n",
    "# rfc = RandomForestClassifier(\n",
    "#     **{'bootstrap': True,\n",
    "#     'class_weight': \"balanced\",\n",
    "#     'criterion': 'gini',\n",
    "#     'max_depth': 3,\n",
    "#     'min_samples_leaf': 19,\n",
    "#     'min_samples_split': 52,\n",
    "#     'n_estimators': 200} )\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    **grid_search_rfc.best_params_\n",
    "                            )\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "knn_pred = knn.predict(Scaled_X_test)\n",
    "svm_pred = svm.predict(Scaled_X_test)\n",
    "rfc_pred = rfc.predict(Scaled_X_test)\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "svm_disp = RocCurveDisplay.from_estimator(svm, Scaled_X_test, y_test, ax=ax, alpha=0.8, color = \"blue\")\n",
    "\n",
    "ax = plt.gca()\n",
    "rfc_disp = RocCurveDisplay.from_estimator(rfc, Scaled_X_test, y_test, ax=ax, alpha=0.8, color = \"orange\")\n",
    "\n",
    "ax = plt.gca()\n",
    "knn_disp = RocCurveDisplay.from_estimator(knn, Scaled_X_test, y_test, ax=ax, alpha=0.8, color = \"green\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"KNN\")\n",
    "KNN_CHART = pd.DataFrame(classification_report(y_test, knn_pred, output_dict=True)).transpose().round(3).loc[[\"0\", \"1\", \"macro avg\"], [\"precision\", \"recall\", \"f1-score\"]]\n",
    "display(KNN_CHART.style.set_caption(\"KNN\"))\n",
    "# display(confusion_matrix(y_test, knn_pred))\n",
    "\n",
    "\n",
    "\n",
    "# display(\"SVM\")\n",
    "SVM_CHART = pd.DataFrame(classification_report(y_test, svm_pred, output_dict=True)).transpose().round(3).loc[[\"0\", \"1\", \"macro avg\"], [\"precision\", \"recall\", \"f1-score\"]]\n",
    "display(SVM_CHART.style.set_caption(\"SVM\"))\n",
    "# display(confusion_matrix(y_test, svm_pred))\n",
    "\n",
    "\n",
    "# display(\"RFC\")\n",
    "RFC_CHART = pd.DataFrame(classification_report(y_test, rfc_pred, output_dict=True)).transpose().round(3).loc[[\"0\", \"1\", \"macro avg\"], [\"precision\", \"recall\", \"f1-score\"]]\n",
    "\n",
    "\n",
    "display(RFC_CHART.style.set_caption(\"RFC\"))\n",
    "# display(confusion_matrix(y_test, rfc_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, knn.predict_proba(Scaled_X_test)[:,1])\n",
    "plt.plot(recall, precision, label = 'Knn', color = \"blue\")\n",
    "plt.xlabel('recall'), plt.ylabel('precision'), plt.title('Knn(n_neighbors = 8) PRC curve')\n",
    "\n",
    "\n",
    "\n",
    "out_arr = np.divide(np.multiply(precision, recall), np.add(precision, recall)) + np.divide(np.multiply(precision, recall), np.add(precision, recall))\n",
    "\n",
    "\n",
    "print(precision[np.where(out_arr == max(out_arr))[0][0]])\n",
    "print(recall[np.where(out_arr == max(out_arr))[0][0]])\n",
    "\n",
    "\n",
    "# print(max(out_arr))\n",
    "\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, svm.decision_function(Scaled_X_test))\n",
    "plt.plot(recall, precision, label = 'Knn', color = \"red\")\n",
    "\n",
    "# , plt.xlabel('recall'), plt.ylabel('precision'), plt.title('svm(n_neighbors = 8) PRC curve')\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, rfc.predict_proba(Scaled_X_test)[:,1])\n",
    "plt.plot(recall, precision, label = 'Knn', color = \"green\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# , plt.xlabel('recall'), plt.ylabel('precision'), plt.title('Knn(n_neighbors = 8) PRC curve'), plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(**grid_k.best_params_)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_pred = knn.predict(Scaled_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d438e6fbbed5d633448b4e14baed9c5a82bf030ec556f60ce2d1496cd792e63c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
